<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="theme" content="Chirpy v2.7.2"><meta name="generator" content="Jekyll v4.2.0" /><meta property="og:title" content="Chapter 2 - Discrete Bayes Filter" /><meta name="author" content="Abhish Khanal" /><meta property="og:locale" content="en_US" /><meta name="description" content="Discrete Bayes Filter" /><meta property="og:description" content="Discrete Bayes Filter" /><link rel="canonical" href="https://aabs7.github.io/posts/Statistics_02/" /><meta property="og:url" content="https://aabs7.github.io/posts/Statistics_02/" /><meta property="og:site_name" content="Abhish Khanal" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2021-01-26T00:27:00+05:45" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Chapter 2 - Discrete Bayes Filter" /><meta name="twitter:site" content="@abheeshkhanal" /><meta name="twitter:creator" content="@Abhish Khanal" /><meta name="google-site-verification" content="google_meta_tag_verification" /> <script type="application/ld+json"> {"description":"Discrete Bayes Filter","url":"https://aabs7.github.io/posts/Statistics_02/","@type":"BlogPosting","headline":"Chapter 2 - Discrete Bayes Filter","dateModified":"2021-01-26T00:27:00+05:45","datePublished":"2021-01-26T00:27:00+05:45","mainEntityOfPage":{"@type":"WebPage","@id":"https://aabs7.github.io/posts/Statistics_02/"},"author":{"@type":"Person","name":"Abhish Khanal"},"@context":"https://schema.org"}</script><title>Chapter 2 - Discrete Bayes Filter | Abhish Khanal</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="preload" href="/assets/css/post.css" as="style"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="/assets/js/post.min.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id="></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', ''); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column align-items-end"><div class="profile-wrapper text-center"><div id="avatar"> <a href="/" alt="avatar" class="mx-auto"> <img src="/assets/img/sample/avatar.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="site-title mt-3"> <a href="/">Abhish Khanal</a></div><div class="site-subtitle font-italic">I am a robotics engineer working in the field of control, estimation, navigation of aerial and ground robots. I love RL.</div></div><ul class="w-100"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/tabs/categories/" class="nav-link"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tabs/tags/" class="nav-link"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/tabs/archives/" class="nav-link"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/tabs/about/" class="nav-link"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul><div class="sidebar-bottom mt-auto d-flex flex-wrap justify-content-center"> <a href="https://github.com/aabs7" aria-label="github" class="order-3" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/abheeshkhanal" aria-label="twitter" class="order-4" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['abheeshkhanal','gmail.com'].join('@')" aria-label="email" class="order-5" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" class="order-6" > <i class="fas fa-rss"></i> </a> <span class="icon-border order-2"></span> <span id="mode-toggle-wrapper" class="order-1"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>Chapter 2 - Discrete Bayes Filter</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>Chapter 2 - Discrete Bayes Filter</h1><div class="post-meta text-muted d-flex flex-column"><div> <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Tue, Jan 26, 2021, 12:27 AM +0545" > Jan 26 <i class="unloaded">2021-01-26T00:27:00+05:45</i> </span> by <span class="author"> Abhish Khanal </span></div><div> <span class="readtime" data-toggle="tooltip" data-placement="bottom" title="5124 words">28 min</span></div></div><div class="post-content"><h1 id="discrete-bayes-filter">Discrete Bayes Filter</h1><style> .output_wrapper, .output { height:auto !important; max-height:100000px; } .output_scroll { box-shadow:none !important; webkit-box-shadow:none !important; }</style><h2 id="tracking-a-dog">Tracking a Dog</h2><p>Let’s begin with a simple problem. We have a dog friendly workspace, and so people bring their dogs to work. Occasionally the dogs wander out of offices and down the halls. We want to be able to track them. So during a hackathon somebody invented a sonar sensor to attach to the dog’s collar. It emits a signal, listens for the echo, and based on how quickly an echo comes back we can tell whether the dog is in front of an open doorway or not. It also senses when the dog walks, and reports in which direction the dog has moved. It connects to the network via wifi and sends an update once a second.</p><p>I want to track my dog Simon, so I attach the device to his collar and then fire up Python, ready to write code to track him through the building. At first blush this may appear impossible. If I start listening to the sensor of Simon’s collar I might read <strong>door</strong>, <strong>hall</strong>, <strong>hall</strong>, and so on. How can I use that information to determine where Simon is?</p><p>To keep the problem small enough to plot easily we will assume that there are only 10 positions in the hallway, which we will number 0 to 9, where 1 is to the right of 0. For reasons that will be clear later, we will also assume that the hallway is circular or rectangular. If you move right from position 9, you will be at position 0.</p><p>When I begin listening to the sensor I have no reason to believe that Simon is at any particular position in the hallway. From my perspective he is equally likely to be in any position. There are 10 positions, so the probability that he is in any given position is 1/10.</p><p>Let’s represent our belief of his position in a NumPy array. I could use a Python list, but NumPy arrays offer functionality that we will be using soon.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">belief</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="o">/</span><span class="mi">10</span><span class="p">]</span><span class="o">*</span><span class="mi">10</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">belief</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]
</pre></table></code></div></div><p>In <a href="https://en.wikipedia.org/wiki/Bayesian_probability">Bayesian statistics</a> this is called a <a href="https://en.wikipedia.org/wiki/Prior_probability"><em>prior</em></a>. It is the probability prior to incorporating measurements or other information. More completely, this is called the <em>prior probability distribution</em>. A <a href="https://en.wikipedia.org/wiki/Probability_distribution"><em>probability distribution</em></a> is a collection of all possible probabilities for an event. Probability distributions always sum to 1 because something had to happen; the distribution lists all possible events and the probability of each.</p><p>I’m sure you’ve used probabilities before - as in “the probability of rain today is 30%”. The last paragraph sounds like more of that. But Bayesian statistics was a revolution in probability because it treats probability as a belief about a single event. Let’s take an example. I know that if I flip a fair coin infinitely many times I will get 50% heads and 50% tails. This is called <a href="https://en.wikipedia.org/wiki/Frequentist_inference"><em>frequentist statistics</em></a> to distinguish it from Bayesian statistics. Computations are based on the frequency in which events occur.</p><p>I flip the coin one more time and let it land. Which way do I believe it landed? Frequentist probability has nothing to say about that; it will merely state that 50% of coin flips land as heads. In some ways it is meaningless to assign a probability to the current state of the coin. It is either heads or tails, we just don’t know which. Bayes treats this as a belief about a single event - the strength of my belief or knowledge that this specific coin flip is heads is 50%. Some object to the term “belief”; belief can imply holding something to be true without evidence. In this book it always is a measure of the strength of our knowledge. We’ll learn more about this as we go.</p><p>Bayesian statistics takes past information (the prior) into account. We observe that it rains 4 times every 100 days. From this I could state that the chance of rain tomorrow is 1/25. This is not how weather prediction is done. If I know it is raining today and the storm front is stalled, it is likely to rain tomorrow. Weather prediction is Bayesian.</p><p>In practice statisticians use a mix of frequentist and Bayesian techniques. Sometimes finding the prior is difficult or impossible, and frequentist techniques rule. In this book we can find the prior. When I talk about the probability of something I am referring to the probability that some specific thing is true given past events. When I do that I’m taking the Bayesian approach.</p><p>Now let’s create a map of the hallway. We’ll place the first two doors close together, and then another door further away. We will use 1 for doors, and 0 for walls:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">hallway</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</pre></table></code></div></div><p>I start listening to Simon’s transmissions on the network, and the first data I get from the sensor is <strong>door</strong>. For the moment assume the sensor always returns the correct answer. From this I conclude that he is in front of a door, but which one? I have no reason to believe he is in front of the first, second, or third door. What I can do is assign a probability to each door. All doors are equally likely, and there are three of them, so I assign a probability of 1/3 to each door.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre><span class="kn">import</span> <span class="nn">kf_book.book_plots</span> <span class="k">as</span> <span class="n">book_plots</span>
<span class="kn">from</span> <span class="nn">kf_book.book_plots</span> <span class="kn">import</span> <span class="n">figsize</span><span class="p">,</span> <span class="n">set_figsize</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">belief</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">book_plots</span><span class="p">.</span><span class="n">bar_plot</span><span class="p">(</span><span class="n">belief</span><span class="p">)</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/statistics_2/output_10_0.png" alt="png" /></p><p>This distribution is called a <a href="https://en.wikipedia.org/wiki/Categorical_distribution"><em>categorical distribution</em></a>, which is a discrete distribution describing the probability of observing \(n\) outcomes. It is a <a href="https://en.wikipedia.org/wiki/Multimodal_distribution"><em>multimodal distribution</em></a> because we have multiple beliefs about the position of our dog. Of course we are not saying that we think he is simultaneously in three different locations, merely that we have narrowed down our knowledge to one of these three locations. My (Bayesian) belief is that there is a 33.3% chance of being at door 0, 33.3% at door 1, and a 33.3% chance of being at door 8.</p><p>This is an improvement in two ways. I’ve rejected a number of hallway positions as impossible, and the strength of my belief in the remaining positions has increased from 10% to 33%. This will always happen. As our knowledge improves the probabilities will get closer to 100%.</p><p>A few words about the <a href="https://en.wikipedia.org/wiki/Mode_%28statistics%29"><em>mode</em></a> of a distribution. Given a list of numbers, such as {1, 2, 2, 2, 3, 3, 4}, the <em>mode</em> is the number that occurs most often. For this set the mode is 2. A distribution can contain more than one mode. The list {1, 2, 2, 2, 3, 3, 4, 4, 4} contains the modes 2 and 4, because both occur three times. We say the former list is <a href="https://en.wikipedia.org/wiki/Unimodality"><em>unimodal</em></a>, and the latter is <em>multimodal</em>.</p><p>Another term used for this distribution is a <a href="https://en.wikipedia.org/wiki/Histogram"><em>histogram</em></a>. Histograms graphically depict the distribution of a set of numbers. The bar chart above is a histogram.</p><p>I hand coded the <code class="language-plaintext highlighter-rouge">belief</code> array in the code above. How would we implement this in code? We represent doors with 1, and walls as 0, so we will multiply the hallway variable by the percentage, like so;</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">belief</span> <span class="o">=</span> <span class="n">hallway</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">belief</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>[0.333 0.333 0.    0.    0.    0.    0.    0.    0.333 0.   ]
</pre></table></code></div></div><h2 id="extracting-information-from-sensor-readings">Extracting Information from Sensor Readings</h2><p>Let’s put Python aside and think about the problem a bit. Suppose we were to read the following from Simon’s sensor:</p><ul><li>door<li>move right<li>door</ul><p>Can we deduce Simon’s location? Of course! Given the hallway’s layout there is only one place from which you can get this sequence, and that is at the left end. Therefore we can confidently state that Simon is in front of the second doorway. If this is not clear, suppose Simon had started at the second or third door. After moving to the right, his sensor would have returned ‘wall’. That doesn’t match the sensor readings, so we know he didn’t start there. We can continue with that logic for all the remaining starting positions. The only possibility is that he is now in front of the second door. Our belief is:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">belief</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>
</pre></table></code></div></div><p>I designed the hallway layout and sensor readings to give us an exact answer quickly. Real problems are not so clear cut. But this should trigger your intuition - the first sensor reading only gave us low probabilities (0.333) for Simon’s location, but after a position update and another sensor reading we know more about where he is. You might suspect, correctly, that if you had a very long hallway with a large number of doors that after several sensor readings and positions updates we would either be able to know where Simon was, or have the possibilities narrowed down to a small number of possibilities. This is possible when a set of sensor readings only matches one to a few starting locations.</p><p>We could implement this solution now, but instead let’s consider a real world complication to the problem.</p><h2 id="noisy-sensors">Noisy Sensors</h2><p>Perfect sensors are rare. Perhaps the sensor would not detect a door if Simon sat in front of it while scratching himself, or misread if he is not facing down the hallway. Thus when I get <strong>door</strong> I cannot use 1/3 as the probability. I have to assign less than 1/3 to each door, and assign a small probability to each blank wall position. Something like</p><pre><code class="language-Python">[.31, .31, .01, .01, .01, .01, .01, .01, .31, .01]
</code></pre><p>At first this may seem insurmountable. If the sensor is noisy it casts doubt on every piece of data. How can we conclude anything if we are always unsure?</p><p>The answer, as for the problem above, is with probabilities. We are already comfortable assigning a probabilistic belief to the location of the dog; now we have to incorporate the additional uncertainty caused by the sensor noise.</p><p>Say we get a reading of <strong>door</strong>, and suppose that testing shows that the sensor is 3 times more likely to be right than wrong. We should scale the probability distribution by 3 where there is a door. If we do that the result will no longer be a probability distribution, but we will learn how to fix that in a moment.</p><p>Let’s look at that in Python code. Here I use the variable <code class="language-plaintext highlighter-rouge">z</code> to denote the measurement. <code class="language-plaintext highlighter-rouge">z</code> or <code class="language-plaintext highlighter-rouge">y</code> are customary choices in the literature for the measurement. As a programmer I prefer meaningful variable names, but I want you to be able to read the literature and/or other filtering code, so I will start introducing these abbreviated names now.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">update_belief</span><span class="p">(</span><span class="n">hall</span><span class="p">,</span> <span class="n">belief</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">correct_scale</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">hall</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">val</span> <span class="o">==</span> <span class="n">z</span><span class="p">:</span>
            <span class="n">belief</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*=</span> <span class="n">correct_scale</span>

<span class="n">belief</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">reading</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># 1 is 'door'
</span><span class="n">update_belief</span><span class="p">(</span><span class="n">hallway</span><span class="p">,</span> <span class="n">belief</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="n">reading</span><span class="p">,</span> <span class="n">correct_scale</span><span class="o">=</span><span class="mf">3.</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'belief:'</span><span class="p">,</span> <span class="n">belief</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'sum ='</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">belief</span><span class="p">))</span>
<span class="n">plt</span><span class="p">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">book_plots</span><span class="p">.</span><span class="n">bar_plot</span><span class="p">(</span><span class="n">belief</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>belief: [0.3 0.3 0.1 0.1 0.1 0.1 0.1 0.1 0.3 0.1]
sum = 1.6000000000000003
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/statistics_2/output_17_1.png" alt="png" /></p><p>This is not a probability distribution because it does not sum to 1.0. But the code is doing mostly the right thing - the doors are assigned a number (0.3) that is 3 times higher than the walls (0.1). All we need to do is normalize the result so that the probabilities correctly sum to 1.0. Normalization is done by dividing each element by the sum of all elements in the list. That is easy with NumPy:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">belief</span> <span class="o">/</span> <span class="nb">sum</span><span class="p">(</span><span class="n">belief</span><span class="p">)</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>array([0.188, 0.188, 0.062, 0.062, 0.062, 0.062, 0.062, 0.062, 0.188,
       0.062])
</pre></table></code></div></div><p>FilterPy implements this with the <code class="language-plaintext highlighter-rouge">normalize</code> function:</p><pre><code class="language-Python">from filterpy.discrete_bayes import normalize
normalize(belief)
</code></pre><p>It is a bit odd to say “3 times as likely to be right as wrong”. We are working in probabilities, so let’s specify the probability of the sensor being correct, and compute the scale factor from that. The equation for that is</p>\[scale = \frac{prob_{correct}}{prob_{incorrect}} = \frac{prob_{correct}} {1-prob_{correct}}\]<p>Also, the <code class="language-plaintext highlighter-rouge">for</code> loop is cumbersome. As a general rule you will want to avoid using <code class="language-plaintext highlighter-rouge">for</code> loops in NumPy code. NumPy is implemented in C and Fortran, so if you avoid for loops the result often runs 100x faster than the equivalent loop.</p><p>How do we get rid of this <code class="language-plaintext highlighter-rouge">for</code> loop? NumPy lets you index arrays with boolean arrays. You create a boolean array with logical operators. We can find all the doors in the hallway with:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">hallway</span> <span class="o">==</span> <span class="mi">1</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre>array([ True,  True, False, False, False, False, False, False,  True,
       False])
</pre></table></code></div></div><p>When you use the boolean array as an index to another array it returns only the elements where the index is <code class="language-plaintext highlighter-rouge">True</code>. Thus we can replace the <code class="language-plaintext highlighter-rouge">for</code> loop with</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">belief</span><span class="p">[</span><span class="n">hall</span><span class="o">==</span><span class="n">z</span><span class="p">]</span> <span class="o">*=</span> <span class="n">scale</span>
</pre></table></code></div></div><p>and only the elements which equal <code class="language-plaintext highlighter-rouge">z</code> will be multiplied by <code class="language-plaintext highlighter-rouge">scale</code>.</p><p>Teaching you NumPy is beyond the scope of this book. I will use idiomatic NumPy constructs and explain them the first time I present them. If you are new to NumPy there are many blog posts and videos on how to use NumPy efficiently and idiomatically.</p><p>Here is our improved version:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">filterpy.discrete_bayes</span> <span class="kn">import</span> <span class="n">normalize</span>

<span class="k">def</span> <span class="nf">scaled_update</span><span class="p">(</span><span class="n">hall</span><span class="p">,</span> <span class="n">belief</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">z_prob</span><span class="p">):</span> 
    <span class="n">scale</span> <span class="o">=</span> <span class="n">z_prob</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">z_prob</span><span class="p">)</span>
    <span class="n">belief</span><span class="p">[</span><span class="n">hall</span><span class="o">==</span><span class="n">z</span><span class="p">]</span> <span class="o">*=</span> <span class="n">scale</span>
    <span class="n">normalize</span><span class="p">(</span><span class="n">belief</span><span class="p">)</span>

<span class="n">belief</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">]</span> <span class="o">*</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">scaled_update</span><span class="p">(</span><span class="n">hallway</span><span class="p">,</span> <span class="n">belief</span><span class="p">,</span> <span class="n">z</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">z_prob</span><span class="o">=</span><span class="p">.</span><span class="mi">75</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'sum ='</span><span class="p">,</span> <span class="nb">sum</span><span class="p">(</span><span class="n">belief</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'probability of door ='</span><span class="p">,</span> <span class="n">belief</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">'probability of wall ='</span><span class="p">,</span> <span class="n">belief</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
<span class="n">book_plots</span><span class="p">.</span><span class="n">bar_plot</span><span class="p">(</span><span class="n">belief</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">.</span><span class="mi">3</span><span class="p">))</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
</pre><td class="rouge-code"><pre>sum = 1.0
probability of door = 0.1875
probability of wall = 0.06249999999999999
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/statistics_2/output_23_1.png" alt="png" /></p><p>We can see from the output that the sum is now 1.0, and that the probability of a door vs wall is still three times larger. The result also fits our intuition that the probability of a door must be less than 0.333, and that the probability of a wall must be greater than 0.0. Finally, it should fit our intuition that we have not yet been given any information that would allow us to distinguish between any given door or wall position, so all door positions should have the same value, and the same should be true for wall positions.</p><p>This result is called the <a href="https://en.wikipedia.org/wiki/Posterior_probability"><em>posterior</em></a>, which is short for <em>posterior probability distribution</em>. All this means is a probability distribution <em>after</em> incorporating the measurement information (posterior means ‘after’ in this context). To review, the <em>prior</em> is the probability distribution before including the measurement’s information.</p><p>Another term is the <a href="https://en.wikipedia.org/wiki/Likelihood_function"><em>likelihood</em></a>. When we computed <code class="language-plaintext highlighter-rouge">belief[hall==z] *= scale</code> we were computing how <em>likely</em> each position was given the measurement. The likelihood is not a probability distribution because it does not sum to one.</p><p>The combination of these gives the equation</p>\[\mathtt{posterior} = \frac{\mathtt{likelihood} \times \mathtt{prior}}{\mathtt{normalization}}\]<p>When we talk about the filter’s output we typically call the state after performing the prediction the <em>prior</em> or <em>prediction</em>, and we call the state after the update either the <em>posterior</em> or the <em>estimated state</em>.</p><p>It is very important to learn and internalize these terms as most of the literature uses them extensively.</p><p>Does <code class="language-plaintext highlighter-rouge">scaled_update()</code> perform this computation? It does. Let me recast it into this form:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">scaled_update</span><span class="p">(</span><span class="n">hall</span><span class="p">,</span> <span class="n">belief</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">z_prob</span><span class="p">):</span> 
    <span class="n">scale</span> <span class="o">=</span> <span class="n">z_prob</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">z_prob</span><span class="p">)</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">hall</span><span class="p">))</span>
    <span class="n">likelihood</span><span class="p">[</span><span class="n">hall</span><span class="o">==</span><span class="n">z</span><span class="p">]</span> <span class="o">*=</span> <span class="n">scale</span>
    <span class="k">return</span> <span class="n">normalize</span><span class="p">(</span><span class="n">likelihood</span> <span class="o">*</span> <span class="n">belief</span><span class="p">)</span>
</pre></table></code></div></div><p>This function is not fully general. It contains knowledge about the hallway, and how we match measurements to it. We always strive to write general functions. Here we will remove the computation of the likelihood from the function, and require the caller to compute the likelihood themselves.</p><p>Here is a full implementation of the algorithm:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">likelihood</span><span class="p">,</span> <span class="n">prior</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">normalize</span><span class="p">(</span><span class="n">likelihood</span> <span class="o">*</span> <span class="n">prior</span><span class="p">)</span>
</pre></table></code></div></div><p>Computation of the likelihood varies per problem. For example, the sensor might not return just 1 or 0, but a <code class="language-plaintext highlighter-rouge">float</code> between 0 and 1 indicating the probability of being in front of a door. It might use computer vision and report a blob shape that you then probabilistically match to a door. It might use sonar and return a distance reading. In each case the computation of the likelihood will be different. We will see many examples of this throughout the book, and learn how to perform these calculations.</p><h2 id="terminology">Terminology</h2><p>The <em>system</em> is what we are trying to model or filter. Here the system is our dog. The <em>state</em> is its current configuration or value. In this chapter the state is our dog’s position. We rarely know the actual state, so we say our filters produce the <em>estimated state</em> of the system. In practice this often gets called the state, so be careful to understand the context.</p><p>One cycle of prediction and updating with a measurement is called the state or system <em>evolution</em>, which is short for <em>time evolution</em> [7]. Another term is <em>system propagation</em>. It refers to how the state of the system changes over time. For filters, time is usually a discrete step, such as 1 second. For our dog tracker the system state is the position of the dog, and the state evolution is the position after a discrete amount of time has passed.</p><p>We model the system behavior with the <em>process model</em>. Here, our process model is that the dog moves one or more positions at each time step. This is not a particularly accurate model of how dogs behave. The error in the model is called the <em>system error</em> or <em>process error</em>.</p><p>The prediction is our new <em>prior</em>. Time has moved forward and we made a prediction without benefit of knowing the measurements.</p><p>Let’s work an example. The current position of the dog is 17 m. Our epoch is 2 seconds long, and the dog is traveling at 15 m/s. Where do we predict he will be in two seconds?</p><p>Clearly,</p>\[\begin{aligned} \bar x &amp;= 17 + (15*2) \\ &amp;= 47 \end{aligned}\]<p>I use bars over variables to indicate that they are priors (predictions). We can write the equation for the process model like this:</p>\[\bar x_{k+1} = f_x(\bullet) + x_k\]<p>\(x_k\) is the current position or state. If the dog is at 17 m then \(x_k = 17\)</p><p>\(f_x(\bullet)\) is the state propagation function for x. It describes how much the \(x_k\) changes over one time step. For our example it performs the computation \(15 \cdot 2\) so we would define it as</p>\[f_x(v_x, t) = v_k t\]<h2 id="adding-uncertainty-to-the-prediction">Adding Uncertainty to the Prediction</h2><p>All sensors have noise. What if the sensor reported that our dog moved one space, but he actually moved two spaces, or zero? This may sound like an insurmountable problem, but let’s model it and see what happens.</p><p>Assume that the sensor’s movement measurement is 80% likely to be correct, 10% likely to overshoot one position to the right, and 10% likely to undershoot to the left. That is, if the movement measurement is 4 (meaning 4 spaces to the right), the dog is 80% likely to have moved 4 spaces to the right, 10% to have moved 3 spaces, and 10% to have moved 5 spaces.</p><p>Each result in the array now needs to incorporate probabilities for 3 different situations. For example, consider the reported movement of 2. If we are 100% certain the dog started from position 3, then there is an 80% chance he is at 5, and a 10% chance for either 4 or 6. Let’s try coding that:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">predict_move</span><span class="p">(</span><span class="n">belief</span><span class="p">,</span> <span class="n">move</span><span class="p">,</span> <span class="n">p_under</span><span class="p">,</span> <span class="n">p_correct</span><span class="p">,</span> <span class="n">p_over</span><span class="p">):</span>
    <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">belief</span><span class="p">)</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="n">prior</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">belief</span><span class="p">[(</span><span class="n">i</span><span class="o">-</span><span class="n">move</span><span class="p">)</span> <span class="o">%</span> <span class="n">n</span><span class="p">]</span>   <span class="o">*</span> <span class="n">p_correct</span> <span class="o">+</span>
            <span class="n">belief</span><span class="p">[(</span><span class="n">i</span><span class="o">-</span><span class="n">move</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">n</span><span class="p">]</span> <span class="o">*</span> <span class="n">p_over</span> <span class="o">+</span>
            <span class="n">belief</span><span class="p">[(</span><span class="n">i</span><span class="o">-</span><span class="n">move</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">n</span><span class="p">]</span> <span class="o">*</span> <span class="n">p_under</span><span class="p">)</span>      
    <span class="k">return</span> <span class="n">prior</span>

<span class="n">belief</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">]</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">predict_move</span><span class="p">(</span><span class="n">belief</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">.</span><span class="mi">1</span><span class="p">,</span> <span class="p">.</span><span class="mi">8</span><span class="p">,</span> <span class="p">.</span><span class="mi">1</span><span class="p">)</span>
<span class="n">book_plots</span><span class="p">.</span><span class="n">plot_belief_vs_prior</span><span class="p">(</span><span class="n">belief</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/statistics_2/output_34_0.png" alt="png" /></p><p>It appears to work correctly. Now what happens when our belief is not 100% certain?</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
</pre><td class="rouge-code"><pre><span class="n">belief</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">.</span><span class="mi">4</span><span class="p">,</span> <span class="p">.</span><span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">predict_move</span><span class="p">(</span><span class="n">belief</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">.</span><span class="mi">1</span><span class="p">,</span> <span class="p">.</span><span class="mi">8</span><span class="p">,</span> <span class="p">.</span><span class="mi">1</span><span class="p">)</span>
<span class="n">book_plots</span><span class="p">.</span><span class="n">plot_belief_vs_prior</span><span class="p">(</span><span class="n">belief</span><span class="p">,</span> <span class="n">prior</span><span class="p">)</span>
<span class="n">prior</span>
</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>array([0.  , 0.  , 0.  , 0.04, 0.38, 0.52, 0.06, 0.  , 0.  , 0.  ])
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/statistics_2/output_36_1.png" alt="png" /></p><p>Here the results are more complicated, but you should still be able to work it out in your head. The 0.04 is due to the possibility that the 0.4 belief undershot by 1. The 0.38 is due to the following: the 80% chance that we moved 2 positions (0.4 \(\times\) 0.8) and the 10% chance that we undershot (0.6 \(\times\) 0.1). Overshooting plays no role here because if we overshot both 0.4 and 0.6 would be past this position.</p><p>If you look at the probabilities after performing the update you might be dismayed. In the example above we started with probabilities of 0.4 and 0.6 in two positions; after performing the update the probabilities are not only lowered, but they are strewn out across the map.</p><p>This is not a coincidence, or the result of a carefully chosen example - it is always true of the prediction. If the sensor is noisy we lose some information on every prediction. Suppose we were to perform the prediction an infinite number of times - what would the result be? If we lose information on every step, we must eventually end up with no information at all, and our probabilities will be equally distributed across the <code class="language-plaintext highlighter-rouge">belief</code> array. Let’s try this with 100 iterations. The plot is animated; use the slider to change the step number.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
</pre><td class="rouge-code"><pre><span class="n">belief</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
<span class="n">predict_beliefs</span> <span class="o">=</span> <span class="p">[]</span>
    
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">belief</span> <span class="o">=</span> <span class="n">predict_move</span><span class="p">(</span><span class="n">belief</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">.</span><span class="mi">1</span><span class="p">,</span> <span class="p">.</span><span class="mi">8</span><span class="p">,</span> <span class="p">.</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">predict_beliefs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">belief</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">'Final Belief:'</span><span class="p">,</span> <span class="n">belief</span><span class="p">)</span>

</pre></table></code></div></div><div class="language-plaintext highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>Final Belief: [0.104 0.103 0.101 0.099 0.097 0.096 0.097 0.099 0.101 0.103]
</pre></table></code></div></div><p>After 100 iterations we will lose almost all information, even though we were 100% sure that we started in position 0. Feel free to play with the numbers to see the effect of differing number of updates. For example, after 100 updates a small amount of information is left, after 50 a lot is left, but by 200 iterations essentially all information is lost.</p><h2 id="generalizing-with-convolution">Generalizing with Convolution</h2><p>We made the assumption that the movement error is at most one position. But it is possible for the error to be two, three, or more positions. As programmers we always want to generalize our code so that it works for all cases.</p><p>This is easily solved with <a href="https://en.wikipedia.org/wiki/Convolution"><em>convolution</em></a>. Convolution modifies one function with another function. In our case we are modifying a probability distribution with the error function of the sensor. The implementation of <code class="language-plaintext highlighter-rouge">predict_move()</code> is a convolution, though we did not call it that. Formally, convolution is defined as</p>\[(f \ast g) (t) = \int_0^t \!f(\tau) \, g(t-\tau) \, \mathrm{d}\tau\]<p>where \(f\ast g\) is the notation for convolving f by g. It does not mean multiply.</p><p>Integrals are for continuous functions, but we are using discrete functions. We replace the integral with a summation, and the parenthesis with array brackets.</p>\[(f \ast g) [t] = \sum\limits_{\tau=0}^t \!f[\tau] \, g[t-\tau]\]<p>Comparison shows that <code class="language-plaintext highlighter-rouge">predict_move()</code> is computing this equation - it computes the sum of a series of multiplications.</p><p><a href="https://www.khanacademy.org/math/differential-equations/laplace-transform/convolution-integral/v/introduction-to-the-convolution">Khan Academy</a> [4] has a good introduction to convolution, and Wikipedia has some excellent animations of convolutions [5]. But the general idea is already clear. You slide an array called the <em>kernel</em> across another array, multiplying the neighbors of the current cell with the values of the second array. In our example above we used 0.8 for the probability of moving to the correct location, 0.1 for undershooting, and 0.1 for overshooting. We make a kernel of this with the array <code class="language-plaintext highlighter-rouge">[0.1, 0.8, 0.1]</code>. All we need to do is write a loop that goes over each element of our array, multiplying by the kernel, and summing the results. To emphasize that the belief is a probability distribution I have named it <code class="language-plaintext highlighter-rouge">pdf</code>.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre><td class="rouge-code"><pre><span class="k">def</span> <span class="nf">predict_move_convolution</span><span class="p">(</span><span class="n">pdf</span><span class="p">,</span> <span class="n">offset</span><span class="p">,</span> <span class="n">kernel</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pdf</span><span class="p">)</span>
    <span class="n">kN</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">kernel</span><span class="p">)</span>
    <span class="n">width</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">kN</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">prior</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span> <span class="p">(</span><span class="n">kN</span><span class="p">):</span>
            <span class="n">index</span> <span class="o">=</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="p">(</span><span class="n">width</span><span class="o">-</span><span class="n">k</span><span class="p">)</span> <span class="o">-</span> <span class="n">offset</span><span class="p">)</span> <span class="o">%</span> <span class="n">N</span>
            <span class="n">prior</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">pdf</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">prior</span>
</pre></table></code></div></div><p>This illustrates the algorithm, but it runs very slow. SciPy provides a convolution routine <code class="language-plaintext highlighter-rouge">convolve()</code> in the <code class="language-plaintext highlighter-rouge">ndimage.filters</code> module. We need to shift the pdf by <code class="language-plaintext highlighter-rouge">offset</code> before convolution; <code class="language-plaintext highlighter-rouge">np.roll()</code> does that. The move and predict algorithm can be implemented with one line:</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre><span class="n">convolve</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">roll</span><span class="p">(</span><span class="n">pdf</span><span class="p">,</span> <span class="n">offset</span><span class="p">),</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s">'wrap'</span><span class="p">)</span>
</pre></table></code></div></div><p>FilterPy implements this with <code class="language-plaintext highlighter-rouge">discrete_bayes</code>’ <code class="language-plaintext highlighter-rouge">predict()</code> function.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
</pre><td class="rouge-code"><pre><span class="kn">from</span> <span class="nn">filterpy.discrete_bayes</span> <span class="kn">import</span> <span class="n">predict</span>

<span class="n">belief</span> <span class="o">=</span> <span class="p">[.</span><span class="mi">05</span><span class="p">,</span> <span class="p">.</span><span class="mi">05</span><span class="p">,</span> <span class="p">.</span><span class="mi">05</span><span class="p">,</span> <span class="p">.</span><span class="mi">05</span><span class="p">,</span> <span class="p">.</span><span class="mi">55</span><span class="p">,</span> <span class="p">.</span><span class="mi">05</span><span class="p">,</span> <span class="p">.</span><span class="mi">05</span><span class="p">,</span> <span class="p">.</span><span class="mi">05</span><span class="p">,</span> <span class="p">.</span><span class="mi">05</span><span class="p">,</span> <span class="p">.</span><span class="mi">05</span><span class="p">]</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">belief</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="p">[.</span><span class="mi">1</span><span class="p">,</span> <span class="p">.</span><span class="mi">8</span><span class="p">,</span> <span class="p">.</span><span class="mi">1</span><span class="p">])</span>
<span class="n">book_plots</span><span class="p">.</span><span class="n">plot_belief_vs_prior</span><span class="p">(</span><span class="n">belief</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.6</span><span class="p">))</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="/assets/img/statistics_2/output_45_0.png" alt="png" /></p><p>All of the elements are unchanged except the middle ones. The values in position 4 and 6 should be \((0.1 \times 0.05)+ (0.8 \times 0.05) + (0.1 \times 0.55) = 0.1\)</p><p>Position 5 should be \((0.1 \times 0.05) + (0.8 \times 0.55)+ (0.1 \times 0.05) = 0.45\)</p><p>Let’s ensure that it shifts the positions correctly for movements greater than one and for asymmetric kernels.</p><div class="language-python highlighter-rouge"><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="n">prior</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">belief</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="p">[.</span><span class="mi">05</span><span class="p">,</span> <span class="p">.</span><span class="mi">05</span><span class="p">,</span> <span class="p">.</span><span class="mi">6</span><span class="p">,</span> <span class="p">.</span><span class="mi">2</span><span class="p">,</span> <span class="p">.</span><span class="mi">1</span><span class="p">])</span>
<span class="n">book_plots</span><span class="p">.</span><span class="n">plot_belief_vs_prior</span><span class="p">(</span><span class="n">belief</span><span class="p">,</span> <span class="n">prior</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.6</span><span class="p">))</span>
</pre></table></code></div></div><p><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-src="output_47_0.png" alt="png" /></p><p>The position was correctly shifted by 3 positions and we give more weight to the likelihood of an overshoot vs an undershoot, so this looks correct.</p><p>Make sure you understand what we are doing. We are making a prediction of where the dog is moving, and convolving the probabilities to get the prior.</p><p>If we weren’t using probabilities we would use this equation that I gave earlier:</p>\[\bar x_{k+1} = x_k + f_{\mathbf x}(\bullet)\]<p>The prior, our prediction of where the dog will be, is the amount the dog moved plus his current position. The dog was at 10, he moved 5 meters, so he is now at 15 m. It couldn’t be simpler. But we are using probabilities to model this, so our equation is:</p>\[\bar{ \mathbf x}_{k+1} = \mathbf x_k \ast f_{\mathbf x}(\bullet)\]<p>We are <em>convolving</em> the current probabilistic position estimate with a probabilistic estimate of how much we think the dog moved. It’s the same concept, but the math is slightly different. \(\mathbf x\) is bold to denote that it is an array of numbers.</p><h2 id="bayes-theorem-and-the-total-probability-theorem">Bayes Theorem and the Total Probability Theorem</h2><p>We developed the math in this chapter merely by reasoning about the information we have at each moment. In the process we discovered <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem"><em>Bayes’ Theorem</em></a> and the <a href="https://en.wikipedia.org/wiki/Law_of_total_probability"><em>Total Probability Theorem</em></a>.</p><p>Bayes theorem tells us how to compute the probability of an event given previous information.</p><p>We implemented the <code class="language-plaintext highlighter-rouge">update()</code> function with this probability calculation:</p>\[\mathtt{posterior} = \frac{\mathtt{likelihood}\times \mathtt{prior}}{\mathtt{normalization\, factor}}\]<p>We haven’t developed the mathematics to discuss Bayes yet, but this is Bayes’ theorem. Every filter in this book is an expression of Bayes’ theorem. In the next chapter we will develop the mathematics, but in many ways that obscures the simple idea expressed in this equation:</p>\[updated\,knowledge = \big\|likelihood\,of\,new\,knowledge\times prior\, knowledge \big\|\]<p>where \(\| \cdot\|\) expresses normalizing the term.</p><p>We came to this with simple reasoning about a dog walking down a hallway. Yet, as we will see the same equation applies to a universe of filtering problems. We will use this equation in every subsequent chapter.</p><p>Likewise, the <code class="language-plaintext highlighter-rouge">predict()</code> step computes the total probability of multiple possible events. This is known as the <em>Total Probability Theorem</em> in statistics, and we will also cover this in the next chapter after developing some supporting math.</p><p>For now I need you to understand that Bayes’ theorem is a formula to incorporate new information into existing information.</p><h2 id="drawbacks-and-limitations">Drawbacks and Limitations</h2><p>This is a robust and complete filter, and you may use the code in real world solutions. If you need a multimodal, discrete filter, this filter works.</p><p>With that said, this filter it is not used often because it has several limitations. Getting around those limitations is the motivation behind the chapters in the rest of this book.</p><ul><li><p>The first problem is scaling. Our dog tracking problem used only one variable, \(pos\), to denote the dog’s position. Most interesting problems will want to track several things in a large space. Realistically, at a minimum we would want to track our dog’s \((x,y)\) coordinate, and probably his velocity \((\dot{x},\dot{y})\) as well. We have not covered the multidimensional case, but instead of an array we use a multidimensional grid to store the probabilities at each discrete location. Each <code class="language-plaintext highlighter-rouge">update()</code> and <code class="language-plaintext highlighter-rouge">predict()</code> step requires updating all values in the grid, so a simple four variable problem would require \(O(n^4)\) running time <em>per time step</em>. Realistic filters can have 10 or more variables to track, leading to exorbitant computation requirements.</p><li><p>The second problem is that the filter is discrete, but we live in a continuous world. The histogram requires that you model the output of your filter as a set of discrete points. A 100 meter hallway requires 10,000 positions to model the hallway to 1cm accuracy. So each update and predict operation would entail performing calculations for 10,000 different probabilities. It gets exponentially worse as we add dimensions. A 100x100 m\(^2\) courtyard requires 100,000,000 bins to get 1cm accuracy.</p><li><p>A third problem is that the filter is multimodal. In the last example we ended up with strong beliefs that the dog was in position 4 or 9. This is not always a problem. Particle filters, which we will study later, are multimodal and are often used because of this property. But imagine if the GPS in your car reported to you that it is 40% sure that you are on D street, and 30% sure you are on Willow Avenue.</p><li><p>A forth problem is that it requires a measurement of the change in state. We need a motion sensor to detect how much the dog moves. There are ways to work around this problem, but it would complicate the exposition of this chapter, so, given the aforementioned problems, I will not discuss it further.</p></ul><h2 id="references">References</h2><ul><li>[1] D. Fox, W. Burgard, and S. Thrun. “Monte carlo localization: Efficient position estimation for mobile robots.” In <em>Journal of Artifical Intelligence Research</em>, 1999.</ul><p>http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume11/fox99a-html/jair-localize.html</p><ul><li>[2] Dieter Fox, et. al. “Bayesian Filters for Location Estimation”. In <em>IEEE Pervasive Computing</em>, September 2003.</ul><p>http://swarmlab.unimaas.nl/wp-content/uploads/2012/07/fox2003bayesian.pdf</p><ul><li>[3] Sebastian Thrun. “Artificial Intelligence for Robotics”.</ul><p>https://www.udacity.com/course/cs373</p><ul><li>[4] Khan Acadamy. “Introduction to the Convolution”</ul><p>https://www.khanacademy.org/math/differential-equations/laplace-transform/convolution-integral/v/introduction-to-the-convolution</p><ul><li>[5] Wikipedia. “Convolution”</ul><p>http://en.wikipedia.org/wiki/Convolution</p><ul><li><p>[6] Wikipedia. “Law of total probability”</p><p>http://en.wikipedia.org/wiki/Law_of_total_probability</p><li><p>[7] Wikipedia. “Time Evolution”</p></ul><p>https://en.wikipedia.org/wiki/Time_evolution</p><ul><li>[8] We need to rethink how we teach statistics from the ground up</ul><p>http://www.statslife.org.uk/opinion/2405-we-need-to-rethink-how-we-teach-statistics-from-the-ground-up</p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/statistics/'>Statistics</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/statistics/" class="post-tag no-text-decoration" >statistics</a> <a href="/tags/bayes/" class="post-tag no-text-decoration" >bayes</a> <a href="/tags/filter/" class="post-tag no-text-decoration" >filter</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Chapter 2 - Discrete Bayes Filter - Abhish Khanal&url=https://aabs7.github.io/posts/Statistics_02/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Chapter 2 - Discrete Bayes Filter - Abhish Khanal&u=https://aabs7.github.io/posts/Statistics_02/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=Chapter 2 - Discrete Bayes Filter - Abhish Khanal&url=https://aabs7.github.io/posts/Statistics_02/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/Statistics_01/">Chapter 1 - Fundamental concept of statistics</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/robotics/">robotics</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/bayes/">bayes</a> <a class="post-tag" href="/tags/filter/">filter</a> <a class="post-tag" href="/tags/random-variable/">random variable</a> <a class="post-tag" href="/tags/splat/">splat</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/Statistics_01/"><div class="card-body"> <span class="timeago small" > Jan 25 <i class="unloaded">2021-01-25T00:27:00+05:45</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Chapter 1 - Fundamental concept of statistics</h3><div class="text-muted small"><p> Random variable: Each time you roll a dice, the outcome will be from 1 to 6. If we roll a dice 10 times, the probability of getting an one is 1/6. Here the combination of values and associated pr...</p></div></div></a></div><div class="card"> <a href="/posts/weighted_least_squares/"><div class="card-body"> <span class="timeago small" > Jan 7 <i class="unloaded">2021-01-07T07:51:00+05:45</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Chapter 2 - Weighted Least square method</h3><div class="text-muted small"><p> Weighted least square Suppose we take measurements with multiple multimeters, some of which are better than others. For general linear measurement model for m measurements and n unknowns: \(y = Hx...</p></div></div></a></div><div class="card"> <a href="/posts/least_squares/"><div class="card-body"> <span class="timeago small" > Jan 7 <i class="unloaded">2021-01-07T00:27:00+05:45</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Chapter 1 - Least square method</h3><div class="text-muted small"><p> Least squares method of estimation The most probable value of the unknown quantities will be that in which the sum of the squares of the differences between the actually observed and the computed v...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/Statistics_01/" class="btn btn-outline-primary"><p>Chapter 1 - Fundamental concept of statistics</p></a> <span class="btn btn-outline-primary disabled"><p>-</p></span></div></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2021 <a href="https://twitter.com/abheeshkhanal">Abhish Khanal</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/robotics/">robotics</a> <a class="post-tag" href="/tags/statistics/">statistics</a> <a class="post-tag" href="/tags/bayes/">bayes</a> <a class="post-tag" href="/tags/filter/">filter</a> <a class="post-tag" href="/tags/random-variable/">random variable</a> <a class="post-tag" href="/tags/splat/">splat</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://aabs7.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
